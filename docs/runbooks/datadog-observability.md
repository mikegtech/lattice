# Runbook: Datadog Observability Operations

Operational procedures for managing Lattice observability in Datadog.

## Quick Links

- [Lattice Worker Health Dashboard](https://app.datadoghq.com/dashboard/xxx-xxx-xxx)
- [All Lattice Logs](https://app.datadoghq.com/logs?query=service%3Alattice-worker-*)
- [DLQ Logs](https://app.datadoghq.com/logs?query=%40event%3Alattice.message.dlq)
- [Monitors List](https://app.datadoghq.com/monitors/manage?q=tag%3Aproject%3Alattice)
- [Terraform Configuration](../../../infra/datadog/)

---

## Daily Operations

### Check Dashboard Health

1. Open Lattice Worker Health dashboard
2. Verify:
   - Active Workers count matches expected (6 workers)
   - Errors (1h) is within acceptable range (<10 for normal operation)
   - DLQ (24h) is 0 or near-zero
   - No red/orange conditional formatting on widgets

### Review Active Alerts

```
Datadog → Monitors → Manage Monitors → Filter: project:lattice status:alert
```

---

## Responding to Alerts

### DLQ Spike Detected

**Alert**: `[Lattice] DLQ Spike Detected`

**Investigation Steps**:

1. **Identify error pattern**:
   ```
   Datadog Logs → @event:lattice.message.dlq
   Group by: @error_code
   ```

2. **Check if single service or widespread**:
   ```
   @event:lattice.message.dlq | Group by: service
   ```

3. **Review recent deployments**:
   - Check GitHub Actions for recent deploys
   - Check if error started after a specific commit

4. **Common causes and fixes**:

   | Error Code | Cause | Fix |
   |------------|-------|-----|
   | `SCHEMA_VALIDATION` | Producer sending malformed data | Check upstream service |
   | `ETIMEDOUT` | Downstream service unavailable | Check dependent services |
   | `RATE_LIMITED` | External API rate limit hit | Review backoff config |
   | `EMBEDDING_SERVICE_UNAVAILABLE` | Embedding service down | Check embedding service health |

5. **Resolution**:
   - Fix root cause
   - Replay DLQ messages if needed (see [mail-deletion runbook](./mail-deletion.md))

---

### Worker No Logs

**Alert**: `[Lattice] No Logs from Worker - {{service.name}}`

**Investigation Steps**:

1. **Check pod status**:
   ```bash
   kubectl get pods -l app={{service.name}} -n lattice
   kubectl describe pod -l app={{service.name}} -n lattice
   ```

2. **Check for OOM kills**:
   ```bash
   kubectl get events -n lattice --sort-by='.lastTimestamp' | grep OOM
   ```

3. **Check Datadog Agent**:
   ```bash
   kubectl logs -l app=datadog-agent -n datadog --tail=100
   ```

4. **Check if intentional**:
   - Was there a scale-down?
   - Is there a deployment in progress?

5. **Resolution**:
   - If crash loop: Check logs, increase resources
   - If OOM: Increase memory limits
   - If agent issue: Restart Datadog agent

---

### High Processing Latency

**Alert**: `[Lattice] High Processing Latency - {{service.name}}`

**Investigation Steps**:

1. **Check which operations are slow**:
   ```
   @event:lattice.message.processed service:{{service.name}} @duration_ms:>2000
   ```

2. **Check resource utilization**:
   ```bash
   kubectl top pods -l app={{service.name}} -n lattice
   ```

3. **Check dependent services**:
   - Database query times
   - Embedding service latency
   - Milvus upsert times

4. **Common causes**:
   - Large email payloads
   - Database connection pool exhaustion
   - GC pressure (check memory usage)
   - External service degradation

5. **Resolution**:
   - Scale up if resource-bound
   - Optimize slow queries
   - Review message payload sizes

---

### Kafka Connection Errors

**Alert**: `[Lattice] Kafka Connection Errors`

**Investigation Steps**:

1. **Check Confluent Cloud status**: https://status.confluent.cloud

2. **Verify credentials**:
   ```bash
   kubectl get secret kafka-credentials -n lattice -o yaml
   ```

3. **Check all workers**:
   ```
   @event:lattice.kafka.error | Group by: service
   ```
   - If all workers affected: Cluster-wide issue
   - If single worker: Pod-specific issue

4. **Check network policies**:
   ```bash
   kubectl get networkpolicies -n lattice
   ```

5. **Resolution**:
   - If cluster issue: Wait for Confluent resolution or failover
   - If credentials: Rotate and redeploy
   - If network: Review network policy rules

---

## Infrastructure Operations

### Deploying Terraform Changes

1. **Create PR with changes** to `infra/datadog/`

2. **Review plan** in PR comment (auto-generated by workflow)

3. **Merge to main** → Terraform apply runs automatically

4. **Verify deployment**:
   - Check GitHub Actions run
   - Check Datadog for new/modified resources
   - Look for deployment success event:
     ```
     source:github status:success service:terraform
     ```

### Adding a New Monitor

1. Create new `.tf` file in `infra/datadog/` (e.g., `monitor_new_feature.tf`)

2. Follow monitor template:
   ```hcl
   resource "datadog_monitor" "new_monitor" {
     name = "[Lattice] Descriptive Name"
     type = "log alert"  # or "metric alert"

     query = "your query here"

     message = <<-EOT
   ## Alert Title

   Description of what happened.

   ### Investigation Steps
   1. Step one
   2. Step two

   ${local.notify_slack_alerts}
   EOT

     monitor_thresholds {
       critical = 10
       warning  = 5
     }

     tags = concat(local.common_tags, [
       "severity:medium",
       "category:new-category"
     ])
   }
   ```

3. Create PR, review plan, merge

### Adding Dashboard Widget

1. Edit `infra/datadog/dashboard.tf`

2. Add widget to appropriate group:
   ```hcl
   widget {
     query_value_definition {
       title = "New Metric"
       request {
         log_query {
           index = "*"
           search_query = "your query"
           compute {
             aggregation = "count"
           }
         }
       }
     }
   }
   ```

3. Deploy via PR → merge workflow

### Modifying Log Pipeline

1. Edit `infra/datadog/pipelines.tf`

2. **Processor order matters** - new processors should be added at the correct position

3. Test query in Datadog Logs first to verify expected behavior

4. Deploy and verify logs are processed correctly

---

## Troubleshooting

### Logs Not Appearing in Datadog

1. **Check `dd.forward` attribute**:
   ```
   # Search for logs that might be filtered
   service:lattice-worker-* @dd.forward:false
   ```

2. **Check tier configuration**:
   - DEBUG tier is not shipped by default
   - Review `logging.shipTiers` in worker config

3. **Check Datadog Agent**:
   ```bash
   docker logs lattice-datadog-agent 2>&1 | grep -i error
   ```

4. **Check container labels**:
   ```bash
   docker inspect lattice-mail-parser | grep -A 20 Labels
   ```

### Monitor Not Triggering

1. **Test query manually** in Datadog Logs Explorer

2. **Check time window** matches expected data frequency

3. **Verify monitor is enabled**:
   ```
   Monitors → Manage → Find monitor → Check "Enabled" toggle
   ```

4. **Check notification channel**:
   - Is Slack integration configured?
   - Is the channel name correct?

### Dashboard Widgets Empty

1. **Check time range** - expand to last 24h

2. **Check template variables** - ensure filters match existing data

3. **Verify query** in Logs Explorer

4. **Check index** - ensure logs are being indexed (not excluded)

---

## Maintenance

### Weekly Tasks

- [ ] Review DLQ count for the week
- [ ] Check for any ignored/muted monitors
- [ ] Review error code distribution
- [ ] Check dashboard for anomalies

### Monthly Tasks

- [ ] Review and tune alert thresholds
- [ ] Clean up unused monitors
- [ ] Update runbook with new learnings
- [ ] Review Datadog costs and optimize if needed

### Quarterly Tasks

- [ ] Audit monitor coverage
- [ ] Review dashboard relevance
- [ ] Update Terraform provider version
- [ ] Review log retention settings

---

## Emergency Procedures

### All Monitors Firing

1. **Don't panic** - check for common cause
2. Check Confluent Cloud status
3. Check GCP/AWS status
4. Check recent deployments
5. If infrastructure issue: Wait/failover
6. If deployment issue: Rollback

### Datadog Outage

1. Check https://status.datadoghq.com
2. Workers continue to function (logs buffer locally)
3. Alert on-call via backup channel (phone/SMS)
4. Monitor Datadog status for resolution

### Terraform State Corruption

1. **Do not run apply**
2. Check GCS bucket for state versions:
   ```bash
   gsutil ls -la gs://YOUR_PROJECT_ID-terraform-state/datadog/
   ```
3. Restore previous version:
   ```bash
   gsutil cp gs://YOUR_PROJECT_ID-terraform-state/datadog/default.tfstate#VERSION ./
   ```
4. Import resources if needed:
   ```bash
   terraform import datadog_monitor.monitor_name MONITOR_ID
   ```
